{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "409d27520da0"
   },
   "source": [
    "# Applying BlueFog on Deep Learning problem(High Level API Introduction)\n",
    "\n",
    "All previous sections we focused on the low-level API in the BlueFog, which is great for flexible algorithm design and research. Give a quick summary here:\n",
    "\n",
    "- Basic static topology propertis and how to manipulate it\n",
    "- Collective communication such as broadcast and allreduce\n",
    "- Topology based Neighborhood communication such as neighbor_allreduce.\n",
    "- Blocking versus non-blocking operation\n",
    "- Dynamic topology and its corresponding\n",
    "- Asynchronous operation through window object\n",
    "- Several decentralized algorithms and their performance under different scenarios\n",
    "- etc.\n",
    "\n",
    "However, it can be boilerplate if you want to apply one certain algorithm on different tasks. Further, it is also tricky to write a efficiency code combining above mentioned concepts. This becomes even worse in the deep learning problem. Backpropagation property of neural network makes that the gradient can be efficiently calculated. Backpropagation also implied that the gradient is calculated (approximately) in layer-by-layer style, in contrast to one global stochastic (sub-)gradient we encountered in the optimization. Further, this layer-wise computation provides a great opportunity to overlap the communication and comptuation for minimizing the trainning time, which is a crucial. Clearly, writing the code to address these concern correctly and efficient is not easy. Hence, BlueFog further provides the high level APIs, which can be directly applied on the `torch.Optimizer` directly. \n",
    "\n",
    "In this section, we will focus in applying high level APIs of BlueFog on Deep Learning problem, mainly the decentralized trainning task.\n",
    "Before we demystify how we implement the High-Level API in BlueFog, let's see the example of using them to write distributed trainning of ResNet-18 model over CIFAR-10 easily. (*Note, although this example is relative small, it still can be time-consuming and drain tons of computation resources if you want to train them on CPUs.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "16e02cc90976"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluefog/miniconda3/envs/bf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import ipyparallel as ipp\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "579523c07451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ipp.Client(profile=\"bluefog\")\n",
    "rc.block = True\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc6185ac461f"
   },
   "source": [
    "## Prepared the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b287fd7db903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/bluefog/bluefog/examples/../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:18<00:00, 9219506.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/bluefog/bluefog/examples/../data/cifar-10-python.tar.gz to /home/bluefog/bluefog/examples/../data\n"
     ]
    }
   ],
   "source": [
    "# Down the CIFAR10 Dataset if not available\n",
    "# Since the dataset is smaller enough, we just load it in-memory.\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    os.path.join(os.getcwd(), \"..\", \"data\"),\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6db4fe3f6cfe"
   },
   "outputs": [],
   "source": [
    "# Distribute the data into each worker.\n",
    "# Note we push the full dataset into each worker is just for simplicity.\n",
    "# Each worker only read the partial of dataset later.\n",
    "_ = rc[:].push({\"train_dataset\": train_dataset, \"val_dataset\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5282f167c4f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] using cuda.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] using cuda.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] using cuda.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] using cuda.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import math\n",
    "import torch\n",
    "import bluefog.torch as bf\n",
    "from bluefog.common import topology_util\n",
    "\n",
    "seed = 42\n",
    "bf.init()\n",
    "torch.manual_seed(seed)\n",
    "run_on_cuda = torch.cuda.is_available()\n",
    "if run_on_cuda:\n",
    "    print(\"using cuda.\")\n",
    "    # Bluefog: pin GPU to local rank.\n",
    "    device_id = (\n",
    "        bf.local_rank()\n",
    "        if bf.nccl_built()\n",
    "        else bf.local_rank() % torch.cuda.device_count()\n",
    "    )\n",
    "    torch.cuda.set_device(device_id)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "else:\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "59190bd2f1ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:2] /home/bluefog/miniconda3/envs/bf/lib/python3.10/site-packages/torch/__init__.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] /home/bluefog/miniconda3/envs/bf/lib/python3.10/site-packages/torch/__init__.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] /home/bluefog/miniconda3/envs/bf/lib/python3.10/site-packages/torch/__init__.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] /home/bluefog/miniconda3/envs/bf/lib/python3.10/site-packages/torch/__init__.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4311e103526c"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Prepare the distributed loader for dataset.\n",
    "batch_size = 32\n",
    "val_batch_size = 1024\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if run_on_cuda else {}\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset, num_replicas=bf.size(), rank=bf.rank()\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs\n",
    ")\n",
    "\n",
    "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    val_dataset, num_replicas=bf.size(), rank=bf.rank()\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_batch_size, sampler=val_sampler, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c324644d036"
   },
   "source": [
    "## Set up standard torch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6cb62e20c52f"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(num_classes=10)\n",
    "if run_on_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Scale learning rate by the number of GPUs.\n",
    "base_lr = 0.0125\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00005\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=(base_lr * bf.size()),\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db3803bdc39d"
   },
   "source": [
    "## Wrap the torch standard optimizer into BlueFog distributed one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0016e16b8e2e"
   },
   "outputs": [
    {
     "ename": "NoEnginesRegistered",
     "evalue": "This operation requires engines. Try client.wait_for_engines(n) to wait for engines to register.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoEnginesRegistered\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43matc_style = False\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbase_dist_optimizer = (\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    bf.DistributedAdaptThenCombineOptimizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if atc_style\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    else bf.DistributedAdaptWithCombineOptimizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43moptimizer = base_dist_optimizer(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    optimizer, model=model, communication_type=bf.CommunicationType.neighbor_allreduce\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Bluefog: broadcast parameters & optimizer state.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbf.broadcast_parameters(model.state_dict(), root_rank=0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbf.broadcast_optimizer_state(optimizer, root_rank=0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/magics.py:476\u001b[0m, in \u001b[0;36mParallelMagics.cell_px\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    474\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39mblock\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     ar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroupby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_after_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignal_on_interrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignal_on_interrupt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtargets:\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/magics.py:380\u001b[0m, in \u001b[0;36mParallelMagics.parallel_execute\u001b[0;34m(self, cell, block, groupby, save_name, stream_output, progress_after, signal_on_interrupt)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mprint\u001b[39m(base \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m execution on engine(s): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m str_targets)\n\u001b[0;32m--> 380\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m result\u001b[38;5;241m.\u001b[39m_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_result \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/view.py:55\u001b[0m, in \u001b[0;36msync_results\u001b[0;34m(f, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_sync_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_sync_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/view.py:39\u001b[0m, in \u001b[0;36msave_ids\u001b[0;34m(f, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m n_previous \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     nmsgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mhistory) \u001b[38;5;241m-\u001b[39m n_previous\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/view.py:658\u001b[0m, in \u001b[0;36mDirectView.execute\u001b[0;34m(self, code, silent, targets, block)\u001b[0m\n\u001b[1;32m    655\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m block\n\u001b[1;32m    656\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m targets\n\u001b[0;32m--> 658\u001b[0m _idents, _targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m futures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ident \u001b[38;5;129;01min\u001b[39;00m _idents:\n",
      "File \u001b[0;32m~/miniconda3/envs/bf/lib/python3.10/site-packages/ipyparallel/client/client.py:699\u001b[0m, in \u001b[0;36mClient._build_targets\u001b[0;34m(self, targets)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ids:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# flush notification socket if no engines yet, just in case\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids:\n\u001b[0;32m--> 699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNoEnginesRegistered(\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt build targets without any engines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m         )\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ids\n",
      "\u001b[0;31mNoEnginesRegistered\u001b[0m: This operation requires engines. Try client.wait_for_engines(n) to wait for engines to register."
     ]
    }
   ],
   "source": [
    "%%px\n",
    "atc_style = False\n",
    "base_dist_optimizer = (\n",
    "    bf.DistributedAdaptThenCombineOptimizer\n",
    "    if atc_style\n",
    "    else bf.DistributedAdaptWithCombineOptimizer\n",
    ")\n",
    "optimizer = base_dist_optimizer(\n",
    "    optimizer, model=model, communication_type=bf.CommunicationType.neighbor_allreduce\n",
    ")\n",
    "\n",
    "# Bluefog: broadcast parameters & optimizer state.\n",
    "bf.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "bf.broadcast_optimizer_state(optimizer, root_rank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f83fffb8187c"
   },
   "source": [
    "## Define several helper functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "424fe7146cc1"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "def accuracy(output, target):\n",
    "    # get the index of the max log-probability\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    return pred.eq(target.view_as(pred)).cpu().float().mean()\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sum = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "        self.n = torch.tensor(0.0)  # pylint: disable=not-callable\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += bf.allreduce(val.detach().cpu(), name=self.name)\n",
    "        self.n += 1\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "\n",
    "dynamic_neighbor_allreduce_gen = topology_util.GetDynamicOnePeerSendRecvRanks(\n",
    "    bf.load_topology(), bf.rank()\n",
    ")\n",
    "\n",
    "\n",
    "def dynamic_topology_update(epoch, batch_idx):\n",
    "    send_neighbors, recv_neighbors = next(dynamic_neighbor_allreduce_gen)\n",
    "    optimizer.send_neighbors = send_neighbors\n",
    "    optimizer.neighbor_weights = {\n",
    "        r: 1 / (len(recv_neighbors) + 1) for r in recv_neighbors\n",
    "    }\n",
    "    optimizer.self_weight = 1 / (len(recv_neighbors) + 1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(epoch, batch_idx):\n",
    "    if epoch < 5:  # warmup_epochs\n",
    "        epoch += float(batch_idx + 1) / len(train_loader)\n",
    "        lr_adj = 1.0 / bf.size() * (epoch * (bf.size() - 1) / 5 + 1)\n",
    "    elif epoch < 30:\n",
    "        lr_adj = 1.0\n",
    "    elif epoch < 60:\n",
    "        lr_adj = 1e-1\n",
    "    elif epoch < 80:\n",
    "        lr_adj = 1e-2\n",
    "    else:\n",
    "        lr_adj = 1e-3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = base_lr * bf.size() * lr_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0e5522361fa3"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# publish_data is useful for monitoring the status of workers\n",
    "from ipyparallel.datapub import publish_data\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_loss = Metric(\"train_loss\")\n",
    "    train_accuracy = Metric(\"train_accuracy\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        adjust_learning_rate(epoch, batch_idx)\n",
    "        dynamic_topology_update(epoch, batch_idx)\n",
    "\n",
    "        if run_on_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_accuracy.update(accuracy(output, target))\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        train_loss.update(loss)\n",
    "        # Average gradients among sub-batches\n",
    "        loss.div_(math.ceil(float(len(data)) / batch_size))\n",
    "        loss.backward()\n",
    "        # Gradient is applied across all ranks\n",
    "        optimizer.step()\n",
    "        publish_data(\n",
    "            dict(\n",
    "                batch_idx=batch_idx,\n",
    "                loss=train_loss.avg.item(),\n",
    "                accuracy=100.0 * train_accuracy.avg.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    val_loss = Metric(\"val_loss\")\n",
    "    val_accuracy = Metric(\"val_accuracy\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            if run_on_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            val_loss.update(torch.nn.functional.cross_entropy(output, target))\n",
    "            val_accuracy.update(accuracy(output, target))\n",
    "            publish_data(\n",
    "                dict(\n",
    "                    batch_idx=batch_idx,\n",
    "                    loss=val_loss.avg.item(),\n",
    "                    accuracy=100.0 * val_accuracy.avg.item(),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52213f591def"
   },
   "source": [
    "### A fake example of monitoring the status of workers\n",
    "\n",
    "The trainning of deep learning model can be time consuming. In order to monitor the \n",
    "status of workers, you can use `ipyparallel.datapub.publish_data` to push the data\n",
    "of the workers to the notebook. Following is a simple fake example to help you understand\n",
    "how to use publish_data and [tqdm](https://tqdm.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "383b63b8b846"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# publish_data is useful for monitoring the status of workers\n",
    "import time\n",
    "from ipyparallel.datapub import publish_data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fake_training():\n",
    "    for i in range(n_iter):\n",
    "        loss, accuracy = np.random.rand(2)\n",
    "        publish_data(dict(i=i, loss=loss, accuracy=accuracy))\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9197b6c3c1e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch     #2: 100%|██████████| 49/49 [00:25<00:00,  1.95it/s, loss=0.846, accuracy=84.98%]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dview = rc[:]\n",
    "dview.block = True\n",
    "n_iter = 50\n",
    "dview[\"n_iter\"] = n_iter\n",
    "ar = dview.apply_async(lambda: fake_training())\n",
    "epoch = 1\n",
    "\n",
    "with tqdm(total=n_iter - 1, desc=\"Train Epoch     #{}\".format(epoch + 1)) as t:\n",
    "    while not ar.ready():\n",
    "        data = ar.data[0]\n",
    "        t.set_postfix(\n",
    "            {\n",
    "                \"loss\": data.get(\"loss\", 0),\n",
    "                \"accuracy\": f\"{data.get('accuracy', 0)*100:.2f}%\",\n",
    "            }\n",
    "        )\n",
    "        t.update(data.get(\"i\", 0) - t.n)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a16c8852affb"
   },
   "source": [
    "## Start decentralized trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "399b2ca8964b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch     #1: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s, loss=1.72, accuracy=41.46%]\n",
      "Validate Epoch  #1: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s, loss=1.51, accuracy=52.60%]\n",
      "Train Epoch     #2: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s, loss=1.23, accuracy=57.31%]\n",
      "Validate Epoch  #2: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s, loss=1.75, accuracy=58.47%]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    rc[:].push({\"epoch\": epochs}, block=True)\n",
    "\n",
    "    # Training\n",
    "    with tqdm(\n",
    "        total=len(rc[0][\"train_loader\"]) - 1,\n",
    "        desc=\"Train Epoch     #{}\".format(epoch + 1),\n",
    "    ) as t:\n",
    "        ar_train = rc[:].apply_async(lambda: train(epoch))\n",
    "        while not ar_train.ready() or t.n < t.total:\n",
    "            data = ar_train.data[0]\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": data.get(\"loss\", 0),\n",
    "                    \"accuracy\": f\"{data.get('accuracy', 0):.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            t.update(data.get(\"batch_idx\", 0) - t.n)\n",
    "            time.sleep(0.5)\n",
    "    # Validation\n",
    "    with tqdm(\n",
    "        total=len(rc[0][\"val_loader\"]) - 1, desc=\"Validate Epoch  #{}\".format(epoch + 1)\n",
    "    ) as t:\n",
    "        ar_val = rc[:].apply_async(lambda: validate(epoch))\n",
    "        while not ar_val.ready() or t.n < t.total:\n",
    "            data = ar_val.data[0]\n",
    "            t.set_postfix(\n",
    "                {\n",
    "                    \"loss\": data.get(\"loss\", 0),\n",
    "                    \"accuracy\": f\"{data.get('accuracy', 0):.2f}%\",\n",
    "                }\n",
    "            )\n",
    "            t.update(data.get(\"batch_idx\", 0) - t.n)\n",
    "            time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51577e0b594e"
   },
   "source": [
    "# Demystify the BlueFog High Level APIs\n",
    "\n",
    "Here we show more details about two common BlueFog high level APIs -- `DistributedAdaptWithCombineOptimizer` and `DistributedAdaptThenCombineOptimizer`. They are corresponding to ATC and AWC algorithm we introduced before. After understanding how these works, you should be able to quickly grasp main idea of other high level APIs or even build your own high level APIs with different decentralized algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "800ee87c54bb"
   },
   "source": [
    "From the algorithm part, there is nothing new happened under the these two distributed optimizer. But we update our notation slightly since in neural network it is common to denote the gradient layer by layer:\n",
    "\n",
    "\\begin{align}\n",
    "    grad_{l}(w_{k,i}) \n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& \\frac{\\partial f_k(w_{k,i})}{\\partial w_{k,i}[l+1]}\\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]} \\\\\n",
    "     =& grad_{l+1}(w_{k,i}) \\frac{\\partial w_{k,i}[l+1]}{\\partial w_{k,i}[l]}\n",
    "\\end{align}\n",
    "where we use $l$ for $l-$ layer and $[l]$ means the parameter related to layer $l$. In this notation, it is closer to reflect how torch works. Except that, the AWC and ATC are applied in the same way. However, in the engineering part,\n",
    "it is not the same as we did before. The main difference is that we no longer wait for the gradient computation of whole model finished to start the communication. Instead, we try to maximize the communication and computation through layer-wise style. \n",
    "\n",
    "\n",
    "First, let's review the typical deep learning training under Torch framework:\n",
    "```python\n",
    "loss = metric(model(data), target)\n",
    "loss.backward()  <--- # it starts to compute the gradient\n",
    "optimizer.step() <--- # it applies the computed gradient on the weights\n",
    "```\n",
    "The `backwards()` function will inversely traverse the neural networks and compute the gradients of each components in the model. The earliest time of executing communication is either at the *forward* or the *backward* step depending on the communication vector required gradient information or not. The latest time of finishing communication is at the `step()` step.\n",
    "A typical communication and computation timeline for Adapt-With-Combine optimizer and Adapt-Then-Combine optimizer is shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddee22c408a2"
   },
   "source": [
    "<img src=\"atc_awc_timeline.png\" alt=\"atc_awc_timeline.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0c5a5c3b8cf"
   },
   "source": [
    "To achieve that the key mechanism is `hook` pattern or so-called [Template method pattern](https://en.wikipedia.org/wiki/Template_method_pattern). For those who are not familar with this design pattern, we provide a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4032b7a550d7"
   },
   "outputs": [],
   "source": [
    "class TriviaHookPattern:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def register_pre_hook(self, f):\n",
    "        self._pre_hook = f\n",
    "\n",
    "    def register_post_hook(self, f):\n",
    "        self._post_hook = f\n",
    "\n",
    "    def execute(self):\n",
    "        self._pre_hook()\n",
    "        print(\"Execute some function\")\n",
    "        self._post_hook()\n",
    "\n",
    "    def _pre_hook(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _post_hook(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "12230f1cb516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am pre hook\n",
      "Execute some function\n",
      "I am post hook\n"
     ]
    }
   ],
   "source": [
    "h = TriviaHookPattern()\n",
    "h.register_pre_hook(lambda: print(\"I am pre hook\"))\n",
    "h.register_post_hook(lambda: print(\"I am post hook\"))\n",
    "h.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b98c5a0be558"
   },
   "source": [
    "There are three types of hooks provided by torch APIs.\n",
    "\n",
    "- `register_forward_hook` for one `torch.nn` module\n",
    "- `register_backward_hook` for one `torch.nn` module\n",
    "- `register_hook()` in gradient computation of one `torch.Tensor`\n",
    "\n",
    "We implement distributed optimizers with some proper combination of these hooks. We would not list the full source code due to too many engineering details. But we demonstrate a one-layer neural network example to illustrate how to combine it with bluefog non-blocking communication operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "44dac4674ae8"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.num_parameters = input_dim * output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fd4cf34241a5"
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# register for communication hook for model\n",
    "data, target = torch.rand(5), torch.rand(1)\n",
    "model = LinearNet(5, 1)\n",
    "\n",
    "handles = []\n",
    "\n",
    "\n",
    "def make_hook(model):\n",
    "    def hook(model, *unused):\n",
    "        for name, parameter in model.named_parameters():\n",
    "            print(\"Make neighbor_allreduce for \", name)\n",
    "            handle = bf.neighbor_allreduce_nonblocking(parameter.data)\n",
    "            handles.append(handle)\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "hook = make_hook(model)\n",
    "_ = model.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c0c5fa8a9f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:1] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:2] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n",
      "[stdout:3] \n",
      "Make neighbor_allreduce for  fc.weight\n",
      "Make neighbor_allreduce for  fc.bias\n",
      "[50250, 50251]\n",
      "[tensor([[ 0.0330, -0.3186,  0.2665,  0.2265, -0.3765]]), tensor([0.3806])]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "# Start nonblocking communication at first time.\n",
    "# Wait until it finished at last step.\n",
    "y = model(data)\n",
    "loss = nn.MSELoss()(y, target)\n",
    "loss.backward()\n",
    "print(handles)\n",
    "tensors = [bf.wait(handle) for handle in handles]\n",
    "handles.clear()\n",
    "print(tensors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Applying BlueFog on Deep Learning problem(High Level API Introduction).ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
